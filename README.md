# ‚ö° METALlama.cpp - Prepackaged Bare-Metal Install for Llama.cpp

<p align="center">
  <img src="https://raw.githubusercontent.com/sanchez314c/METALlama.cpp/main/.images/metallama-hero.png" alt="METALlama.cpp Hero" width="600" />
</p>

**METALlama.cpp - Prepackaged bare-metal install for Llama.cpp w/ Metal Support on Intel Macs & Hackintosh w/ AMD GPUs**

[![Shell](https://img.shields.io/badge/Shell-Bash-green.svg)](https://www.gnu.org/software/bash/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Metal](https://img.shields.io/badge/Metal-MPS_Acceleration-blue.svg)](https://developer.apple.com/metal/)
[![macOS](https://img.shields.io/badge/macOS-11.0+-blue.svg)](https://www.apple.com/macos/)
[![Intel](https://img.shields.io/badge/Intel-Mac_Support-lightgrey.svg)](https://www.intel.com/)

## üéØ Overview

METALlama.cpp is the ultimate solution for running Llama.cpp with Metal Performance Shaders (MPS) acceleration on Intel Macs with AMD GPUs. While Apple has shifted focus to M-series chips, millions of Intel Macs with discrete AMD graphics are left behind. This project bridges that gap, delivering high-performance AI inference on "forgotten" hardware.

Transform your Intel Mac into a powerful AI workstation with GPU-accelerated inference, OpenAI-compatible API server, and seamless integration with popular AI applications.

## ‚ú® Key Features

### üöÄ **Metal Performance Optimization**
- **GPU Acceleration**: Full Metal/MPS support for AMD discrete graphics
- **Memory Management**: Intelligent VRAM allocation and overflow handling
- **Layer Optimization**: Automatic GPU layer calculation based on available memory
- **Thermal Management**: Built-in temperature monitoring and throttling

### üîß **One-Click Installation**
- **Automated Setup**: Complete llama.cpp build and configuration
- **Dependency Management**: Automatic installation of required tools
- **Model Selection**: Curated collection of GGUF models (1B-8B parameters)
- **Service Integration**: launchd service for automatic startup

### üåê **OpenAI-Compatible API**
- **REST API Server**: Drop-in replacement for OpenAI API endpoints
- **Multi-Application Support**: Works with BoltAI, OpenWebUI, LibreChat, and more
- **Network Access**: Configurable local-only or network-wide access
- **Health Monitoring**: Built-in health checks and status endpoints

### üõ†Ô∏è **Professional Tools**
- **CLI Interface**: Interactive chat and scripting capabilities
- **Service Management**: Start, stop, restart, and monitor services
- **Debug Mode**: Comprehensive logging and troubleshooting
- **Performance Profiling**: GPU utilization and inference benchmarking

## üèóÔ∏è Architecture

```
METALlama.cpp/
‚îú‚îÄ‚îÄ metallama_mps-metal_llamacpp_installer-macos.sh  # Main installer
‚îú‚îÄ‚îÄ run.sh                                          # Quick launch script
‚îú‚îÄ‚îÄ setup.py                                        # Python setup utilities
‚îú‚îÄ‚îÄ docs/                                           # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ API.md                                      # API reference
‚îÇ   ‚îú‚îÄ‚îÄ BENCHMARKS.md                               # Performance data
‚îÇ   ‚îú‚îÄ‚îÄ FAQ.md                                      # Common questions
‚îÇ   ‚îú‚îÄ‚îÄ INSTALL.md                                  # Installation guide
‚îÇ   ‚îî‚îÄ‚îÄ MODELS.md                                   # Model information
‚îú‚îÄ‚îÄ CHANGELOG.md                                    # Version history
‚îú‚îÄ‚îÄ CONTRIBUTING.md                                 # Development guide
‚îú‚îÄ‚îÄ SECURITY.md                                     # Security guidelines
‚îî‚îÄ‚îÄ LICENSE                                         # MIT license
```

## üöÄ Quick Start

### Prerequisites
- **macOS 11.0+** (Big Sur or later for Metal MPS support)
- **Intel Mac with AMD GPU** (discrete graphics required)
- **8GB+ RAM** (16GB+ recommended for larger models)
- **10GB free disk space** (for models and build artifacts)
- **Xcode Command Line Tools** (`xcode-select --install`)

### Installation

```bash
# Clone the repository
git clone https://github.com/sanchez314c/METALlama.cpp.git
cd METALlama.cpp

# Make installer executable
chmod +x metallama_mps-metal_llamacpp_installer-macos.sh

# Run installation with verbose output
./metallama_mps-metal_llamacpp_installer-macos.sh --verbose

# Quick installation (uses defaults)
./metallama_mps-metal_llamacpp_installer-macos.sh
```

### Installation Options
```bash
# Custom model selection
./metallama_mps-metal_llamacpp_installer-macos.sh --model "Llama-3.2-3B-Instruct"

# Secure mode (localhost only)
./metallama_mps-metal_llamacpp_installer-macos.sh --secure

# Dry run (show what would be done)
./metallama_mps-metal_llamacpp_installer-macos.sh --dry-run

# Skip automatic service start
./metallama_mps-metal_llamacpp_installer-macos.sh --no-autostart
```

## üéÆ Usage Examples

### CLI Interface
```bash
# Interactive chat session
~/llama-chat.sh

# Single question
~/llama-chat.sh "Explain quantum computing in simple terms"

# Programming assistance
~/llama-chat.sh "Write a Python function to calculate fibonacci numbers"

# Creative writing
~/llama-chat.sh "Write a short sci-fi story about AI"
```

### Service Management
```bash
# Check service status
~/llama-service.sh status

# Start the service
~/llama-service.sh start

# Stop the service
~/llama-service.sh stop

# Restart the service
~/llama-service.sh restart

# View service logs
~/llama-service.sh logs

# Monitor in real-time
~/llama-service.sh monitor
```

### API Usage
```bash
# Test API connectivity
curl http://127.0.0.1:8080/health

# Simple chat completion
curl -X POST http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Llama-3.2-1B-Instruct",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Streaming response
curl -X POST http://127.0.0.1:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Llama-3.2-1B-Instruct",
    "messages": [{"role": "user", "content": "Tell me a story"}],
    "stream": true
  }'
```

### Python Integration
```python
import requests
import json

# Configure API endpoint
api_base = "http://127.0.0.1:8080/v1"

def chat_with_llama(message: str, model: str = "Llama-3.2-1B-Instruct"):
    """Send a message to METALlama API"""
    response = requests.post(
        f"{api_base}/chat/completions",
        headers={"Content-Type": "application/json"},
        json={
            "model": model,
            "messages": [{"role": "user", "content": message}],
            "temperature": 0.7,
            "max_tokens": 500
        }
    )
    
    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        return f"Error: {response.status_code}"

# Usage
answer = chat_with_llama("What's the capital of France?")
print(answer)

# Streaming example
def stream_chat(message: str):
    """Stream response from METALlama API"""
    response = requests.post(
        f"{api_base}/chat/completions",
        headers={"Content-Type": "application/json"},
        json={
            "model": "Llama-3.2-1B-Instruct",
            "messages": [{"role": "user", "content": message}],
            "stream": True
        },
        stream=True
    )
    
    for line in response.iter_lines():
        if line:
            try:
                data = json.loads(line.decode('utf-8').replace('data: ', ''))
                if data.get("choices"):
                    content = data["choices"][0]["delta"].get("content", "")
                    print(content, end="", flush=True)
            except json.JSONDecodeError:
                continue

# Usage
stream_chat("Write a poem about technology")
```

## üîß Advanced Configuration

### Performance Tuning
```bash
# config.json - Performance configuration
{
  "gpu_layers": 32,           # Number of layers to offload to GPU
  "context_size": 4096,       # Context window size
  "batch_size": 512,          # Batch size for processing
  "threads": 8,               # CPU threads for non-GPU operations
  "rope_freq_base": 10000,    # RoPE frequency base
  "rope_freq_scale": 1.0,     # RoPE frequency scale
  "temperature": 0.7,         # Default temperature
  "top_p": 0.9,              # Top-p sampling
  "top_k": 40,               # Top-k sampling
  "repeat_penalty": 1.1       # Repetition penalty
}
```

### Memory Optimization
```bash
# Optimize for different RAM configurations

# 8GB RAM setup
./metallama_mps-metal_llamacpp_installer-macos.sh --model "Llama-3.2-1B-Instruct" --gpu-layers 16

# 16GB RAM setup  
./metallama_mps-metal_llamacpp_installer-macos.sh --model "Llama-3.2-3B-Instruct" --gpu-layers 32

# 32GB+ RAM setup
./metallama_mps-metal_llamacpp_installer-macos.sh --model "Llama-3.2-8B-Instruct" --gpu-layers 48
```

### Security Configuration
```bash
# Localhost-only access (secure mode)
./metallama_mps-metal_llamacpp_installer-macos.sh --secure

# Network access with authentication
./metallama_mps-metal_llamacpp_installer-macos.sh --auth-token "your-secret-token"

# Custom port configuration
./metallama_mps-metal_llamacpp_installer-macos.sh --port 8081
```

## ü§ñ Application Integration

### BoltAI Integration
```bash
# BoltAI Configuration
# 1. Open BoltAI Settings
# 2. Navigate to "AI Providers" 
# 3. Add new provider:
#    - Name: METALlama
#    - API Type: OpenAI Compatible
#    - Base URL: http://127.0.0.1:8080/v1
#    - Model: Llama-3.2-1B-Instruct
```

### OpenWebUI Integration
```bash
# Docker setup for OpenWebUI
docker run -d -p 3000:8080 \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:8080/v1 \
  -e OPENAI_API_KEY=dummy \
  -v open-webui:/app/backend/data \
  --name open-webui \
  ghcr.io/open-webui/open-webui:main

# Access at http://localhost:3000
```

### LibreChat Integration
```yaml
# LibreChat config.yaml
version: 1.0.5
cache: true

endpoints:
  custom:
    - name: "METALlama"
      apiKey: "dummy"
      baseURL: "http://127.0.0.1:8080/v1"
      models:
        default: ["Llama-3.2-1B-Instruct", "Llama-3.2-3B-Instruct"]
      titleConvo: true
      titleMethod: "completion"
      summarize: false
      summaryMethod: "completion"
      forcePrompt: false
      modelDisplayLabel: "METALlama"
```

### VS Code Extension Integration
```json
// settings.json for Continue.dev extension
{
  "continue.telemetryEnabled": false,
  "continue.models": [
    {
      "title": "METALlama",
      "provider": "openai",
      "model": "Llama-3.2-1B-Instruct",
      "apiKey": "dummy",
      "apiBase": "http://127.0.0.1:8080/v1"
    }
  ]
}
```

## üìä Performance Benchmarks

### Hardware Configurations Tested

| Mac Model | GPU | RAM | Model Size | Tokens/sec | GPU Layers |
|-----------|-----|-----|------------|------------|------------|
| MacBook Pro 16" 2019 | Radeon Pro 5500M 8GB | 16GB | 1B | 45-55 | 16 |
| MacBook Pro 16" 2019 | Radeon Pro 5500M 8GB | 32GB | 3B | 25-35 | 32 |
| iMac 27" 2020 | Radeon Pro 5700 XT 16GB | 32GB | 3B | 55-65 | 32 |
| iMac 27" 2020 | Radeon Pro 5700 XT 16GB | 64GB | 8B | 15-25 | 48 |
| Mac Pro 2019 | Radeon Pro Vega II 32GB | 64GB | 8B | 35-45 | 48 |

### Optimization Results
```bash
# Before optimization (CPU-only)
Model: Llama-3.2-3B-Instruct
Tokens/second: 3-8
Memory usage: 12GB RAM
GPU utilization: 0%

# After METALlama optimization
Model: Llama-3.2-3B-Instruct  
Tokens/second: 25-35
Memory usage: 8GB RAM + 4GB VRAM
GPU utilization: 85-95%
```

## üêõ Troubleshooting

### Common Issues

**Installation Fails**
```bash
# Check system requirements
system_profiler SPHardwareDataType | grep "Model Name"
system_profiler SPDisplaysDataType | grep "Chipset Model"

# Verify Metal support
system_profiler SPDisplaysDataType | grep "Metal"

# Update Xcode Command Line Tools
sudo xcode-select --install
```

**Service Won't Start**
```bash
# Check service status
launchctl list | grep llama

# View service logs  
~/llama-service.sh logs

# Manual debug launch
~/run-llama-direct.sh --verbose
```

**Poor Performance**
```bash
# Check GPU utilization
sudo powermetrics --samplers gpu_power -n 5 -i 1000

# Adjust GPU layers
~/llama-service.sh stop
# Edit ~/.llama/config.json to reduce gpu_layers
~/llama-service.sh start

# Monitor temperature
sudo powermetrics --samplers smc -n 1 | grep -i temp
```

**Memory Issues**
```bash
# Check available memory
vm_stat | head -5

# Reduce model size or context
./metallama_mps-metal_llamacpp_installer-macos.sh --model "Llama-3.2-1B-Instruct"

# Clear cache
rm -rf ~/.llama/cache/*
```

**API Connection Issues**
```bash
# Test local connectivity
curl -v http://127.0.0.1:8080/health

# Check firewall settings
sudo pfctl -sr | grep 8080

# Test network access
curl -v http://$(ipconfig getifaddr en0):8080/health
```

## üîí Security Considerations

### Local Security
- **Default Configuration**: Server binds to localhost only in secure mode
- **No Authentication**: Default setup has no auth (suitable for local use)
- **Process Isolation**: Service runs as user process, not root
- **File Permissions**: Config files have restricted permissions

### Network Security
```bash
# Enable authentication
export LLAMA_API_KEY="your-secure-token"
~/llama-service.sh restart

# Use HTTPS proxy (nginx/Apache)
# Configure reverse proxy with SSL termination

# Firewall rules
sudo pfctl -f /etc/pf.conf  # Add rules to block external access
```

## üìà Roadmap

### Upcoming Features
- [ ] **Web UI**: Browser-based interface for model management
- [ ] **Model Zoo**: Expanded collection of optimized models
- [ ] **Batch Processing**: Support for batch inference jobs
- [ ] **Distributed Computing**: Multi-machine inference clustering
- [ ] **Plugin System**: Extensions for specialized use cases

### Long-term Goals
- [ ] **Windows Support**: Port to Windows with DirectML
- [ ] **Linux Support**: Ubuntu/Debian compatibility
- [ ] **Cloud Integration**: AWS/GCP deployment options
- [ ] **Mobile Apps**: iOS/Android companion applications
- [ ] **Enterprise Features**: Multi-user, RBAC, auditing

## ü§ù Contributing

### Development Setup
```bash
# Clone for development
git clone https://github.com/sanchez314c/METALlama.cpp.git
cd METALlama.cpp

# Install development dependencies
brew install shellcheck shfmt

# Run tests
./tests/run_tests.sh

# Lint shell scripts
shellcheck *.sh
shfmt -d *.sh
```

### Contributing Guidelines
1. **Fork the repository**
2. **Create feature branch**: `git checkout -b feature/amazing-feature`
3. **Test thoroughly**: Run on multiple Mac configurations
4. **Document changes**: Update README and docs
5. **Submit pull request**: Include benchmarks and test results

### Areas for Contribution
- **Model Optimization**: New quantization strategies
- **Performance Tuning**: GPU memory management improvements
- **Hardware Support**: Additional AMD GPU compatibility
- **Documentation**: Tutorials and best practices
- **Testing**: Automated testing across Mac models

## üåç Intel Mac Community

### Why Intel Macs Matter
- **Installed Base**: Millions of Intel Macs still in active use
- **Professional Workflows**: Many creative and development environments
- **Educational Institutions**: Schools and universities with Intel Mac labs
- **Cost Effectiveness**: Powerful AI without new hardware investment

### Community Resources
- **Discord**: [Intel Mac AI Community](https://discord.gg/intel-mac-ai)
- **Reddit**: [r/IntelMacAI](https://reddit.com/r/intelmacai)
- **Stack Overflow**: Tag questions with `intel-mac` and `metal-mps`

## üìû Support & Resources

### Getting Help
- **Documentation**: [Complete Wiki](https://github.com/sanchez314c/METALlama.cpp/wiki)
- **Issues**: [GitHub Issues](https://github.com/sanchez314c/METALlama.cpp/issues)
- **Discussions**: [Community Forum](https://github.com/sanchez314c/METALlama.cpp/discussions)

### Professional Services
- **Installation Support**: Remote assistance for complex setups
- **Performance Optimization**: Custom tuning for specific workflows
- **Enterprise Deployment**: Multi-machine setup and management
- **Training**: Workshops on AI inference optimization

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **Georgi Gerganov**: Creator of llama.cpp, the foundation of this project
- **Apple Metal Team**: For Metal Performance Shaders framework
- **QuantFactory & TheBloke**: For excellent GGUF model quantizations
- **Intel Mac Community**: For testing, feedback, and support
- **AMD**: For maintaining macOS GPU driver support

## üîó Related Projects

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Original llama.cpp implementation
- [Ollama](https://github.com/ollama/ollama) - Alternative local LLM solution
- [LocalAI](https://github.com/go-skynet/LocalAI) - OpenAI-compatible local AI server

---

<p align="center">
  <strong>Revive your Intel Mac with the power of AI ‚ö°</strong><br>
  <sub>Where forgotten hardware meets cutting-edge AI</sub>
</p>

---

**‚≠ê Star this repository if METALlama.cpp accelerates your Intel Mac AI experience!**